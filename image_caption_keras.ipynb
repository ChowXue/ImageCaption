{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import OrderedDict, Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, TimeDistributed, RepeatVector, Merge\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download tokenizer models if needed\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16 import VGG_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_model_weights = '/home/qhduan/Downloads/COCO/vgg16_weights.h5'\n",
    "coco_train = '/home/qhduan/Downloads/COCO/train2014'\n",
    "coco_caption = '/home/qhduan/Downloads/COCO/annotations/captions_train2014.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_name_caption(coco_caption):\n",
    "    captions = json.load(open(coco_caption, 'r'))\n",
    "    image_id_caption = {}\n",
    "    for caption in captions['annotations']:\n",
    "        image_id_caption[caption['image_id']] = caption['caption']\n",
    "    ret = OrderedDict()\n",
    "    for img in captions['images']:\n",
    "        if img['id'] in image_id_caption:\n",
    "            caption = image_id_caption[img['id']]\n",
    "            file_name = img['file_name']\n",
    "            ret[file_name] = caption\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name_caption = get_file_name_caption(coco_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(file_name_caption.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = len(file_name_caption)\n",
    "print('train_size', train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "START = '<start>'\n",
    "END = '<end>'\n",
    "UNK = '<unk>'\n",
    "PAD = '<pad>'\n",
    "min_count = 5\n",
    "max_len = 0\n",
    "vocabulary = Counter()\n",
    "for caption in tqdm(file_name_caption.values(), file=sys.stdout, total=len(file_name_caption)):\n",
    "    sent = word_tokenize(caption)\n",
    "    vocabulary.update(sent)\n",
    "    if len(sent) > max_len: max_len = len(sent) + 2 # add 2 because <start> and <end>\n",
    "vocabulary = [k for k, v in vocabulary.items() if v >= min_count]\n",
    "vocabulary = sorted(list(set(vocabulary)))\n",
    "word_index = OrderedDict()\n",
    "index_word = OrderedDict()\n",
    "for index, word in enumerate([START, END, UNK, PAD] + vocabulary):\n",
    "    word_index[word] = index\n",
    "    index_word[index] = word\n",
    "vocabulary_size = len(word_index)\n",
    "print('vocabulary_size', vocabulary_size)\n",
    "print('max_len', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embedding_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_model = VGG_16()\n",
    "vgg_model.load_weights(vgg_model_weights)\n",
    "vgg_model.layers.pop()\n",
    "vgg_model.layers.pop()\n",
    "vgg_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_model = Sequential()\n",
    "image_model.add(vgg_model)\n",
    "image_model.add(Dense(128, activation='relu'))\n",
    "image_model.add(RepeatVector(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_model = Sequential()\n",
    "language_model.add(Embedding(vocabulary_size, embedding_size, input_length=max_len))\n",
    "language_model.add(GRU(output_dim=128, return_sequences=True))\n",
    "language_model.add(TimeDistributed(Dense(128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Merge([image_model, language_model], mode='concat', concat_axis=1))\n",
    "model.add(GRU(256, return_sequences=False))\n",
    "model.add(Dense(vocabulary_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_flow(file_name_caption, word_index, coco_dir, max_len, vocabulary_size, batch_size):\n",
    "    X_img = []\n",
    "    X_lang = []\n",
    "    Y = []\n",
    "    while True:\n",
    "        for file_name, caption in file_name_caption.items():\n",
    "            path = os.path.join(coco_dir, file_name)\n",
    "            if os.path.exists(path):\n",
    "                img = np.array(Image.open(path))\n",
    "                if len(img.shape) == 3:\n",
    "                    img = imresize(img, (224, 224)).transpose([2, 0, 1])\n",
    "                    sent = [START] + word_tokenize(caption) + [END]\n",
    "                    for i in range(1, len(sent) - 1):\n",
    "                        if len(X_img) == batch_size:\n",
    "                            X_img = np.asarray(X_img)\n",
    "                            X_lang = np.asarray(X_lang)\n",
    "                            Y = np.asarray(Y).reshape([batch_size, vocabulary_size])\n",
    "                            yield [X_img, X_lang], Y\n",
    "                            X_img = []\n",
    "                            X_lang = []\n",
    "                            Y = []\n",
    "                            \n",
    "                        input_sent = sent[:i]\n",
    "                        padding_size = max_len - len(input_sent)\n",
    "                        \n",
    "                        input_sent = input_sent + padding_size * [PAD]\n",
    "                        input_sent_index = []\n",
    "                        for w in input_sent:\n",
    "                            if w in word_index:\n",
    "                                input_sent_index.append(word_index[w])\n",
    "                            else:\n",
    "                                input_sent_index.append(word_index[UNK])\n",
    "                        \n",
    "                        target_word = sent[i]\n",
    "                        if target_word in word_index:\n",
    "                            target_word_index = word_index[target_word]\n",
    "                        else:\n",
    "                            target_word_index = word_index[UNK]\n",
    "                            \n",
    "                        X_img.append(img)\n",
    "                        X_lang.append(input_sent_index)\n",
    "                        y = np.zeros([vocabulary_size])\n",
    "                        y[target_word_index] = 1.0\n",
    "                        Y.append([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for [x_img, x_lang], y in data_flow(file_name_caption, word_index, coco_train, max_len, vocabulary_size, batch_size):\n",
    "    print(x_img.shape, x_lang.shape, y.shape)\n",
    "    i += 1\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    data_flow(file_name_caption, word_index, coco_train, max_len, vocabulary_size, batch_size),\n",
    "    32,\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
